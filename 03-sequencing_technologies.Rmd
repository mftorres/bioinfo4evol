--- 
title: "Bioinformatics for Evolution"
author: "Maria Fernanda Torres Jimenez"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/bioinfo4evol/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is the material for the M.Sc. "Bioinformatics for Evolution" course at Vilnius University - 2023 Autumn. 
# This is a minimal example of using the bookdown package to write a book.
# The HTML output format for this example is bookdown::bs4_book,
# set in the _output.yml file.
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---

# Sequencing technologies and file formats {#seqtech}

**Brief description:**
This notebook will walk you through the different sequence file formats generated by different sequencing technologies. You will learn to identify the metadata and quality information encoded in the files, perform data manipulations, and convert files between formats. We will also go over the SAM/BAM file format, CIGAR strings, information filters, and statistics.

1. Prepare the data
Before we start, we need to download some of the files from the Sequence Read Archive ([SRA](https://www.ncbi.nlm.nih.gov/sra)) using [sra-toolkit](https://hpc.nih.gov/apps/sratoolkit.html) commands. We will first install the tool kit and then get the read data, then, we will run the download jobs in the background using the command `nohup` ([no hang up](https://en.wikipedia.org/wiki/Nohup). `nohup` allows you to continue working on the command line prompt without killing the processes that are running. If you feel adventurous, you can modify the code that downloads the data and run it as a loop.

```bash
cd $HOME

# sra-toolkit for Ubuntu subsystems and Linux
wget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/3.0.6/sratoolkit.3.0.6-ubuntu64.tar.gz

# untar and uncompress
tar xvzf sratoolkit.3.0.6-ubuntu64.tar.gz

# set a variable pointing to the command that we will use
cd $HOME/sratoolkit.3.0.6-ubuntu64/bin
FASTDUMP="$PWD"/fastq-dump

# test the variable
$FASTDUMP --help

# go back to the lectures' folder
# edit the path to reflect the true path for you
cd /mnt/c/Users/username/Documents/folder/

# download samples. This command will keep the prompt busy and you won't be able to work
$FASTDUMP --split-3 DRR353013

# a useful command to run jobs in the "background" is nohup.
# we are also redirecting any stdout message to a file, in case we need to troubleshoot things
nohup $FASTDUMP --split-3 DRR353013 $2>> fastdump.out &
nohup $FASTDUMP --split-3 SRR8369716 $2>> fastdump.out &
nohup $FASTDUMP --split-3 ERR10558085 $2>> fastdump.out &
nohup $FASTDUMP --split-3 DRR351748 $2>> fastdump.out &
nohup $FASTDUMP --split-3 SRR10988554 $2>> fastdump.out &

# check the job is running using ps
ps
```

Explore the files within the folder, you will inmediately notice there are three types with the extensions `fasta`, `fastq, and `gff3`. Let's look at `wolbachia.fasta` first.

<span style="color: #f08f18;">**Question: **</span>How many sequences are there in the file? Append you answer to your `answersL3_name.txt` file using `>>`.

Try checking how long the sequence in the `wolbachia.fasta` file is. That file is an example of an interleaved fasta file, meaning that the sequence is split into lines with the same number of characters instead of keeping the whole sequence in a single line. However, having the sequence in a single line facilitates operations on the sequence. For example, finding the nth nucleotide of the sequence or finding patterns in the sequence. In an interleaved fasta file, you would need to know what is the number of characters per sequence line to be able ot find the nth nucleotide or will have to account for newline characters at any point of the pattern being searched.

We can use a single line awk command to transform an interleaved fasta into one with the sequence in a single line.

```bash
# from https://www.biostars.org/p/9262/#378544

awk '/^>/ { if(NR>1) print "";  printf("%s\n",$0); next; } { printf("%s",$0);}  END {printf("\n");}' < wolbachia.fasta > wolbachiaSL2.fasta

# make sure there are no empty first lines in the linearised fasta
head -n1 wolbachiaSL.fasta
```

<span style="color: #f08f18;">**Question: **</span>How long is the sequence in file? Append you answer to your `answersL3_name.txt` file using `>>`.


Now, check out the `SRR10988554.fastq` file. How does it compare to the `*fasta` file? Which pattern can you identify? Unlike fasta files, fastq files have four lines: The header, the sequence, the annotation line, and the quality line. the latter contains the same about of symbols as the number of nucleotides in the sequence and each symbol (an [ASCII character](https://www.ascii-code.com/)) represents a quality score of the identification of the base called, or in other words, the accuracy of the base calling. That score is known as the [Phred score](https://en.wikipedia.org/wiki/Phred_quality_score)

Imagine that a colleague of yours sent you three files: `SRR10988554.fastq`, `SRR10988554_1.fastq`, and `SRR10988554_2.fastq`. No one remembers how the data was produced or whether it is useful and of good quality. If you figure out what that data are, your colleague will let you use the data for your own projects and would include you on their publication as an acknowledgement to your contributions. So, what do you do?


The first thing is understanding what the three files and how they relate to each other. Just by looking at the size of the files using `ls -lh` we can tell the files with the `*_1.fastq` and `*_2.fastq` extensions are larger.

```bash
# using a wildcard
ls -lh SRR10988554*fastq
```

We can count the number of reads in each file. <span style="color: #f08f18;">**Question: **</span>Which symbol represents the header and read information line? How many reads are in each file? Use `echo` and '>>' to append the name of each file to the `answersL3_name.txt` file, then append the read count using your own code.

You might have noticed that the symbol marking the header line is also an ASCII character used for encoding base quality. How can you correctly count the reads whilst avoiding counting the symbol when it only represents base quality?

You know now how many reads there are in each file but, why do you think your colleague sent you three files? We can check at the header of a couple of reads. The header contains the SRA accession number followed by a `.` and the number of the read, then a code that uniquely identifies that read, and finally, the read's length. We can pick a random read identifier and use `grep` to check what we find:

```bash
grep 'HY5L7LL02J4WEI' SRR10988554*fastq

# and what about this one?
grep 'HY5L7LL02GHFLS' SRR10988554*fastq

# and this one?
grep 'HY5L7LL02FGBMR' SRR10988554*fastq
```

Can you see a pattern? You can at least tell the type of read-paring in the data. Now, we guess which sequencing technology was used for generating the data based on the length of the reads. We can pipe a couple of commands to `grep` the header line, then select only the *field* containing the length information, then sorting the read lengths to obtain the minimum and maximum read lengths. There are several solutions to this problem. <span style="color: #f08f18;">**Question: **</span> Share the code you use to find the minimum and maximum read lengths in all `SRR10988554*fastq` files, as well as the numbers. Append the answers to your `answersL3_name.txt` file.


There are unexpectedly longer reads. Let's find the header and the context lines to check if it is an error or not.

```bash
grep -A 4 'length=1518' SRR10988554*fastq
```

That particular read is extrange. Looks repetitive and the Phred scores are mainly `!`, `*`, `)`. <span style="color: #f08f18;">**Question: **</span>Would you trust that read? what can you tell about it? Append you answer to your `answersL3_name.txt` file using `nano`.

You are about to figure out the sequencing technology for the sample SRR10988554 when you colleague sends you two more samples (and their files): Sample DRR353013 with files `DRR353013_1.fastq` and `DRR353013_2.fastq`, and sample SRR8369716 with file `SRR8369716.fastq`. Have a look at them and figure out which are the likely sequencing technologies used for generating that data.  <span style="color: #f08f18;">**Question: **</span> Which sequencing technologies are used for generating the SRR10988554 and DRR353013 data? Append the answers to your `answersL3_name.txt` file.

You colleague thinks you are pretty clever and you could be of more help. They have sent them a PacBio run for this organism, sample DRR351748 and file `DRR351748.fastq`, for you to map the sort reads of one of the files you already looked at.

got wolbachia reference from ncbi
hlaf class does illumina, half does nanopore, half does pacboi

REF='./GCF_016584425.1/GCF_016584425.1_ASM1658442v1_genomic.fna'
echo $REF
bwa index $REF

minimap2 -t 4 -ax sr $REF DRR353013_1.fastq DRR353013_2.fastq | samtools sort -@ - | samtools view -@ 4 -hb - > wol_DRR353013.bam

Indexing enables fast random access to the data. Create an index using the samtools index command:

samtools index wol_DRR353013.bam

minimap2 -t 4 -ax map-pb $REF DRR351748.fastq | samtools sort -@ - | samtools view -@ 4 -hb - > wol_DRR351748.bam
samtools index wol_DRR351748.bam

minimap2 -t 4 -ax map-ont $REF ERR10558085.fastq | samtools sort -@ - | samtools view -@ 4 -hb - > wol_ERR10558085.bam
samtools index wol_ERR10558085.bam


check out the bam file. Look at the header alone and check the contigs. Then check the latest commands, read groups, etc.
get BAM stats

Sorting is essential for efficient data retrieval:

check out the order of the reads (two reads one after the other with the same name? ordered by name or number?)
samtools sort -n (check reads again) # mention but dont run it, flagstat needs coord sort

You can quickly calculate statistics from BAM files using the samtools flagstat command

for bam in *bam; do samtools flagstat -@ 4 $bam > "${bam}".flagstat; done

it would be smart to run it within nohup:

nohup bash -c 'for bam in *bam; do samtools flagstat -@ 4 $bam > "${bam%.*}".flagstat; done' &

check the processes running with ps. You can also use top

Now, check the *flagstat files generated
How many reads failed the quality filters in each file? How many mapped?

The stats generated by the flagstats command are not very detailed and only concern read flags. We can get more information using the stats command

nohup bash -c 'for stat in *stats; do samtools stats -@ 4 -c 1,2000,1 $bam > "${bam%.*}".stats; done' &

extract the summary (the code is in the file itself)
for example:

for stat in *stats; do echo -e "\nFile: ${stat}"; grep '^SN' $stat | cut -f 2-; done

We can check quickly the percentage of paired reads:

for stat in *stats; do echo -e "\nFile: ${stat}"; grep '^SN' $stat | cut -f 2- | grep '%'; done

Answer: Why two of the samples have 0\% of properly paired reads?

Lets check other stats from the same file. grep 'grep' froom the file and look at all the code in there. Particularly useful are *Insert sizes* and *Read lengths*. Those sound similar but are actually different. Let's compare.

# The columns are: insert size, pairs total, inward oriented pairs, outward oriented pairs, other pairs
for stat in *stats; do echo -e "\nFile: ${stat}"; grep '^IS' $stat | cut -f 2- | head ; done

# The columns are: read length, count
Read lengths. 
for stat in *stats; do echo -e "\nFile: ${stat}"; grep '^RL' $stat | cut -f 2- | head ; done

It would be easier to see the data aggregated in a histogram. We can do that quickly in bash to save time and code, and as a preliminary check. For that, we need to install a simple program:

pip install bashplotlib

Now, let's try again, looking at the insert size column only (second column only)

for stat in *stats; do echo -e "\nFile: ${stat}"; grep '^IS' $stat | cut -f 2 | sort | hist -b 100 -x ; done

for stat in *stats; do echo -e "\nFile: ${stat}"; grep '^RL' $stat | cut -f 2 | sort | hist -b 100 -x ; done

Other interesting stats are *Coverage distribution* and *Mapping qualities*

Read lengths - last fragments. Use `grep ^LRL | cut -f 2-` to extract this part. The columns are: read length, count

Mapping qualities for reads !(UNMAP|SECOND|SUPPL|QCFAIL|DUP). Use `grep ^MAPQ | cut -f 2-` to extract this part. The columns are: mapq, count




---

1. Text-based sequence formats
1.1. Sequence formats
1.1.1 Fasta
1.1.2. Fasta+Qual
1.1.3. Fastq
1.1.3.1. The PHRED score
1.1.3.2. Single sample file
1.1.3.3. Pooled sample file and demultiplexing
1.1.4. qseq
1.1.5. Sequence Read Format (SRF)

1.2. Annotation formats
1.2.1. GFF general
1.2.2. GTF
1.2.3. GFF3

2. Compressed or binary formats
2.1. The SAM/BAM file
2.2. The CRAM file
2.3. BCL Illumina format

3. Chromatogram based sequence formats
3.1. Capillary Sanger chromatograms

5. Hierarchical formats
5.1. Hierarchical Data Format version 5 (HDF5)
5.2. Fast5

5. Legacy formats
6. Summary by sequencing technology.
6.1. Sanger and capillar sequencing
6.2. Roche 545
6.3. SOLiD
IonTorrent
Helicos
Illumina
PAcBio
ONT
