--- 
title: "Bioinformatics for Evolution"
author: "Maria Fernanda Torres Jimenez"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/bioinfo4evol/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is the material for the M.Sc. "Bioinformatics for Evolution" course at Vilnius University - 2023 Autumn. 
# This is a minimal example of using the bookdown package to write a book.
# The HTML output format for this example is bookdown::bs4_book,
# set in the _output.yml file.
biblio-style: apalike
csl: chicago-fullnote-bibliography.csl
---

# Introduction to bioinformatics and reproducibility in science {#cmd}

**Brief description:**
This notebook will walk you through the installation of Linux subsystems and Ubuntu OS and the steps for setting up a working environment. You will learn basic Linux commands from examples focused on analysing biological data, e.g. sequences and their metadata. This is an introductory tutorial for people that is not familiar with the command line.


1. The Linux subsystem (Windows only)
1.1. Installing the Linux subsystem

2. Conda environments
2.1. Installing Conda
2.2. Creating and deleting environments
2.3. Creating environments from files
2.4. Exporting environments

3. Basic Linux commands
3.1. Folder hierarchy # ls cd pwd mv
3.2. Create, inspect, and delete files # nano less head tail pipe tee touch cut script 
3.3. Matching, replacing, and counting patterns # grep sed perl -pe tr
3.4. Variables, loops, and conditionals # for if fi while wait sedgrepvariable
3.5. Compress and dicompress # gzip tar pigz
3.6. Connecting with other machines # ssh scp rsync

4. AWK
1.1. Basic filtering
1.2. Operations

5. Jupyter notebooks

6. Summary

---

1. The Linux subsystem (Windows only)

1.1. Installing the Linux subsystem
Follow these steps if you work on a Windows machine (likely Windows 11 but it works for Windows 10 too). If you have MacOS or Ubuntu/Linux already running in your machine, skip this section.

Right click on the Windows icon in the task bar, click search, search for 'turn windows features on or off'. Open the application.

Look for the options 'windows subsystem for linux' and 'virtual machine platform' and tick the boxes.

Hit OK and restart your machine.

Go again to the windows icon and search for bash. Open the application. If you see an error and a URL, open the link on a web browser and follow the instructions.

---

2. Conda environments
This section takes you through installing conda and managing environments. Documentation about Conda is available [here](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html).

2.1. Installing Conda
Download the Linux version of Conda [here](https://www.anaconda.com/download#downloads). It is important that you get the Linux distribution as you will be installing it in a Linux subsystem (even within windows). Use the `wget` command to retrieve the file from conda's website. The code below will not work easily if you are working under a VU VPN. If that is the case, try the manual download.

```{bash echo = FALSE}
wget --help # check the manual if you want
wget https://repo.anaconda.com/archive/Anaconda3-2023.07-0-Linux-x86_64.sh
```

Now, open the command line and check where in the folders' hierarchy you are. You can do that with the `pwd` command: **print current directory**. You can explore how to use it by adding the `--help` argument.

```{bash echo = FALSE}
pwd --help
```

You are likely to be in a folder called **home**, pretty much your starting folder. But in the case of linux subsystems within windows, that is bash's starting folder and not your main disk's home folder. You need to **change directory** to your **Downloads** folder to find the conda installer.

Then, **list** files to check if the installer is in that folder. The asterisk is used as a [wildcard](https://tldp.org/LDP/GNU-Linux-Tools-Summary/html/x11655.htm).

To run the bash `*sh` script use the `bash` command. Follow the instructions, either pressing ENTER, space for scrolling down the page, or typing "yes".

**Note:** always replace my username with your username e.g. mftorr should be replaced with your username.

```{bash echo = FALSE}
cd /mnt/c/Users/mftorr/Downloads/
ls Ana*sh
bash Anaconda3-2023.07-0-Linux-x86_64.sh
```

Once you are done, close the bash command line interface and open a new one.

2.2. Creating and deleting environments
Conda allows you to create discrete, enclosed environments with specific versions of programs to ensure a smooth interaction. If you are only worried about one particular program, conda will look for the versions of other programs and dependencies (programs that other programs require for working correctly) that works best and avoids conflict.

Every time we run an analysis associated to a publication, we can export the environment to a file that other researchers can use to recreate the environment used for running said analysis. Better yet, if you need to re-install conda, you can quickly use that file to re-install your environments. Conda environments help increase the reproducibility and transparency of your research.

Most of this section is taken from the [conda](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html) documentation, which you can check in more depth if you need to.

Let's create an environment from scratch. we ask `conda` to `create` a new environment with `-n bioevo` name. When prompted, reply 'y' to whether you want to install the packages needed (for now none). Then, activate the environment using `conda activate`.

Once active, you should see it in the left side of the prompt. **Anything that you install whilst an environment is active will only be available for that environment.**

```bash
conda create -n bioevo
conda activate bioevo
conda deactivate # deactivates the environmnet and takes you back to conda base
# anything after a hastag symbol is ignored, not executed, and considered a comment.
```

If we want to install a particular library through conda, e.g. [VCFtools](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3137218/), you can check for the correct channel (locations where the packages are stored) within conda's repository website, e.g. [here](https://anaconda.org/bioconda/vcftools). The commands `conda config --add channels` indicate conda that it needs to include the channles bioconda and conda-forge, locations where most (but not all) libraries for bioinformatics are sitting.

```bash
conda config --add channels bioconda
conda config --add channels conda-forge
conda install -c bioconda vcftools
```

You can check the list of environments installed within conda and their folder location using the command `conda env list`. Once you see the folder location, you can remove the environment by simply deleting the entire folder using `rm` and the arguments `-r`. The command will ask you if are sure you want to delete the entire folder. BE CAREFUL! That command will remove folders, files, data, everything you have in there. Don't drink and `rm`. Alternatively (if you remember the name of your environments), you can use `conda remove`

```bash
conda env list
rm -r /path/to/environment/folder

conda remove --name bioevo --all
```

2.3. Creating environments from files
For reproducibility, sometimes it is desirable to install an environment from a file that someone else has exported. The process is as simple as usong `conda 

You can confirm that all packages and their dependencies were intalled by listing every component within the environment with `conda list` and the name of the environment. The name of a single package can be added at the end, to specifically check its installation and version.

```bash
conda env create -f bioevo_environment.yml

# check for packages
conda list -n bioevo
conda list -n bioevo blast
```

2.4. Exporting environments
You can export and share your own environment. To do so, make sure the environment is active before using `conda env export`. Here, the '>' symbol re-directs the output to a new file called *bioevo_environment.yml*. Every time you run this command and re-direct the output to the file, the old file will be replaced by the new one (if both have the same name). This behaviour is typical of redirecting with '>' but we will cover that later.

```bash
conda activate bioevo # in case your environment is not active
conda env export > bioevo_environmentv2.yml
```

---

3. Basic Linux commands
We have set up an enviroment within which we can work, now the aim is to use biological data to learn basic Linux commands and how to use them within the context of biological data analysis.

First, we will gather data from [NCBI](https://www.ncbi.nlm.nih.gov/), one of the most widely used databases "[of online resources for biological information and data](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6323993/#:~:text=Abstract,published%20in%20life%20science%20journals)". We are also interested in exploring the data available for Lithuanian spiders on NCBI, using as a reference a species list published by [Biteniekytė and Rėlys (2011)](https://lmaleidykla.lt/ojs/index.php/biologija/article/view/1926/828). Our aim is to generate a report that highlights which spider species have data available, which don't, the type of data available and its quality. You are likely to use these commands often, following a similar data processing that will help you have an idea about data availability before you embark in your own projects.

3.1. Folder hierarchy # ls cd pwd mv
Files are stored within folders (or directories, is the same) and folders are nested within other folders, following a hierarchy similar to a tree structure. Using the command line interface, we can navigate through folders by following the branches of the tree, up or down to different levels of the hierarchy.

We can express the **path** from one folder to another in two ways: absolute and relative. **Absolute paths** explicitely indicate the folder names along the hierarchy path, from the current location to the destination folder or file **Relative** paths express the steps along the hierarchy path to reach another destination and assume the user know its current position *relative* to the rest of the structure.

For example, assuming the current structure: `C/Users/mftorr/Documents/` # ADD IMAGE AND FIX PATHS

Then the absolute and relative paths to reach `C/Users/mftorr/Documents/biol4evol/Lectures/Lecture0` are:

**Absolute:** ``
**Relative:** ``

Thus, to move around folders, the first step is to know your **current working directory**, easily done with `pwd`. It is also helpful to understand which files are inside your current folder by **listing** its items. The command `ls` comes in handy, particularly when specific argumens are used. For example, `ls -l` will list all items within a folder, printing to screen the [permissions of a file](https://en.wikipedia.org/wiki/File-system_permissions) (first column from left to right), size of the file (fifth column) and last-modified date. To access the same information in a "human readable" format, you can use `ls -lh`. If you want to explore more options for listing (or in general for other commands), you can use `ls --help`.

**Advanced:** try using `ls` and Linux wildcards to filter which files to list or move.

You can list the contents of a folder if you know the path. Once you confirm that a particular folder is the one you want to move to, you can **change directory** to that path using `cd`. You can also copy (leaves a copy behind) or move (does not, or replaces one version with another) files across folders. Finally, you can create new folders with `mkdir` and the name of the new folder.

``bash
pwd # likely something like /home/user
ls
ls -lh

# replace user with your username
ls /mnt/c/Users/user/Documents/bioinfo4evol/* # lists all files and folders
cd /mnt/c/Users/user/Documents/bioinfo4evol/practicals/lecture0
mkdir testfolder
cp sequence1.gb testfolder/sequence1.gb
mv sequence2.gb testfolder/

# what happened to file1 and file2?
ls -lh
ls testfolder/
```

**TO DO:**
Familiarise yourself with the folders and files in your system. No need to delete anything, just move around and get used to the structure.

3.2. Create, inspect, and delete files # nano less head tail pipe tee touch cut script
Creating, inspecting, and deleting files is relatively easy. For creating a file that you are not writing to inmediately (just creating) `touch filename is enough.

```bash
# replace "name" with your name without special characters
touch answersL0_name.txt # creates the file
```

If you want to create and modify a file manually, you can do it using `nano filename`. [nano](https://en.wikipedia.org/wiki/GNU_nano) is a text editor for the command line interface and it is very useful for changing files in a non-systematic way. You can navigate through a file inside of nano using the keyboard arrows, the PfDn and PgUP keys, and the Home and End keys (and many more). You can write and modify text as usual, just keep in mind that clicking with a mouse is not an option. Once you are happy with the changes, simply follow the sequence for saving the changes in a file with the same name and exiting the editor: `ctrl+x` (Save modified buffer?) `y` (File Name to Write: filename.file) `enter`.

As an exercise, open the `answersL0_name.txt` in `nano`, write your name, save and close the file. **You will have to answer a few questions throughout this tutorial. For each question, open the `answersL0_name.txt` in `nano`, add the question and answers, save and close the file. You will submit your file by the end of the class.**

Other usefull commands for exploring files without modifiying them are `less`, `head`, and `tail`. `less` is the equivalent of opening the file "in view mode". You can navigate through the file with the keyboard arrows, PgUp and PgDn, and the spacebar, and use the kwy `q` to exit the view.

`head` and `tail` show the first and last lines of a file, respectively. By default, both commands show 10 lines, but it is possible tomodify the number of lines with the `-n 50` argument for, for example, 50 lines.

For example, imagine a program throws you an error that says "unrecognised character in line 928th of sequence1.gb". How can you check that line? You can use `head -n 928` and **pipe** the output to `tail`. **"Piping"** (`|`) allows one process to communicate with a subsequent process, it simplifies and makes more efficient the analysis of large amounts of data.

```bash
head -n 928 sequence1.gb | tail -n 1
```

Two more symbols that are very important and omnipresent are `>` and `>>` for creating and appending to files, respectively. For example, `head -n 10 sequence1.gb > sequence1_n10.txt` creates a file called `sequence1_n10.txt` that contains the first 10 lines in the sequence1.gb file. If the file `sequence1_n10.txt` already exists, then that command will replace the old file with the new one conserving no information from the old file. If, instead of creating a new `sequence1_n10.txt` file we want to **append** additional information to it, we can use `>>`. For example, `head -n 10 sequence2.gb >> sequence1_n10.txt` will append the first 10 lines from the file `sequence2.gb` to the already existing `sequence1_n10.txt` file.

<span style="color: #f08f18;">**Question: **</span>What is the 500th line in `sequence1.gb`? And the 500th line in `sequence2.gb`? Append the two lines to your `answersL0_name.txt` file 

We will now start working with data on Lithuanian spiders. We have a species list from [Biteniekytė and Rėlys (2011)](https://lmaleidykla.lt/ojs/index.php/biologija/article/view/1926/828). The `LT_spider_list.txt` file contains the information as directly copy-pasted from the publication's PDF. Our aim is to transform the unorganised information in `LT_spider_list.txt` into a list of species with taxonomic IDs that we will use to gather published sequences available online.

Have a look at the `LT_spider_list.txt` file using `less`, `head`, and `tail`.

<span style="color: #f08f18;">**Question: **</span>What is the 20th line in `LT_spider_list.txt`? Append the line to your `answersL0_name.txt` file

3.3. Matching, replacing, and counting patterns # grep sed perl -pe tr
You might have noticed a pattern from looking at the file. First you find a line starting with "FAMILY" for the spider family and then subsequent lines for every species in that family. We can use `grep` to print the spider families represented in Lithuania. `grep` is a command to search for **matching patterns** in a file. `grep` has very useful arguments that will help you extract key information from very large files, for example: `-v`, `-A`, `-B`. Familiarise yourself with `grep` and its arguments checking `--help`

```bash
grep --help
```

First, let's check the families in the file. Then, use `wc`, or the **word count** command for counting the lines that match "FAMILY". Finally, you can count the number of unique lines that match "FAMILY" (i.e. the lines with the exact same characters, visible or "invisible") using `uniq -c`. Beware, before checking for unique lines, you must `sort` the lines.

```bash
# grep 'pattern' file
# use " instead of ' if the pattern is a variable (more on that later)
grep 'FAMILY' LT_spider_list.txt
grep 'FAMILY' LT_spider_list.txt | wc -l
grep 'FAMILY' LT_spider_list.txt | sort | uniq -c
```

I encourage you to check all the options for `wc`, `sort`, and `uniq`. These are commands that you will be using very frequently.

<span style="color: #f08f18;">**Question: **</span>How many spider families are in `LT_spider_list.txt`? Append the line to your `answersL0_name.txt` file. What do `wc -l` and `uniq -c` do? Manually add your answer to `answersL0_name.txt` using nano.

We can use grep to search **regular expressions** (known as [regex](https://en.wikipedia.org/wiki/Regular_expression)) to match patterns instead of exact words. For example, we can match a pattern that is made of two strings of alphabetic characters separated by a space character, each string can be of unlimited length as long as one alphabetic character comes after another.

```bash
grep -E '[[:alpha:]]+ [[:alpha:]]+ ' LT_spider_list.txt
grep -E '[A-Za-z]+ [A-Za-z]+' LT_spider_list.txt

# the following will only show the matches, one match per line (two matches in a single input line are in different output lines)
grep -o -E '[A-Za-z]+ [A-Za-z]+' LT_spider_list.txt
```

The code above returns what is mostly "genus species" matches, but there are lines that correspond to words describing species in the original file. We need to remove those. One obvious pattern that emerges is matches like "as speciesX". One option to deal with this is using the **stream editor** `sed`. `sed` is an incredibly powerful command that does match replacement and removes lines by index number or pattern. In the first example, we will use sed to remove lines starting with a pattern `^as ` (where `^` indicates that the following characters must be at the begining of the line).

```bash
# sed pattern file, only in this case we don't specify the file - input is comming from the pipe
grep -o -E '[A-Za-z]+ [A-Za-z]+' LT_spider_list.txt | sed '^as ' 
```

However, that solution is not the best when lines have more than one matching text, i.e., more than one instance with two strings of alphabetic characters in the same line. Other solution can be instead to use `sed` and regex to match the first instance of the pattern and delete anything else afterwards (line by line).

```bash
# s stands for substitute (the command within sed)
# we define groups or variables of patterns using the parentheses
# we call back the groups by index (order in which they ocurr)
# anything other character (symbolised by .)
# g stands for global - replace all matches, not just the first
sed -r 's/^([A-Za-z]+) ([A-Za-z]+).+/\1 \2/g' LT_spider_list.txt

# better yet - we can delete lines matching "FAMILY" first, then removing anything after "genus species" matches
# then remove lines with any punctuation symbol
sed '/FAMILY/d' LT_spider_list.txt | sed -r 's/^([A-Za-z]+) ([A-Za-z]+).+/\1 \2/g' | sed '/[:punct:]/d'
```

We can confirm how good the processing of the file is by sorting the lines and looking at unique strings. In other words, sort the lines to look for things that do not look like "genus species". In bioinformatics (and in life in general) is always good to check and double check your results/output. Particularly if you use match/replace, since there will be instances that you did not expect.

```bash
# sort and count unique, then pass output to a file
sed '/FAMILY/d' LT_spider_list.txt | sed -r 's/^([A-Za-z]+) ([A-Za-z]+).+/\1 \2/g' | sed '/[[:punct:]]/d' | sort | uniq -c > species.txt
```

You will notice that `uniq -c` produces two columns, the first column is the frequency of a string and the second column is the string itself. You can **cut** the columns in a file and look to specific ones using `cut` and specifiying a field delimiter (like in Excel) using `cut -d" "`. In this case, the delimiter is any space (generally includes tabs, spaces, etc). You will also notice that `uniq -c` adds a TAB (or four spaces) before the first column, you can replace multiple spaces with a single one by **translating** the strings using `tr`. `cat` simply prints the file contents to the screen.

```bash
# print second column if delimiter is a single space
# cut shows the fields 3 and 4.
cat species.txt | tr -s ' ' | cut -d' ' -f3,4

# the above transformation results in the same output as the code below
cut -d' ' -f8,9 species.txt
```

Similarly, to check the unique genera in the list, we can combine `cat`, `tr`, `sort`, and `unique`

```bash
cat species.txt | tr -s ' ' | cut -d' ' -f3 | sort | uniq > genera.txt
```

<span style="color: #f08f18;">**Question: **</span>What is happening? Is it working perfectly or not? How many genera is there? How could you process this file more efficiently? Manually add your answer to `answersL0_name.txt` using nano.

To speed up the process of sequence downloads, we will select the first 10 genera.

**TO DO:**
Create a file called `genera_filtered.txt` with the first 10 genera from `genera.txt`. We will search for [COI]() sequences for the genera in the list by processing one line at a time. We will use loops and variables to achieve it.

3.4. Variables, loops, and conditionals # for if fi while wait sedgrepvariable
A variable is a symbolic name that representes a value that can be asigned and re-assigned depending on the context they are used. Variables are useful for generalising programs, creating pipelines where only one imput/argument changes, or for making loops more efficiently.

You can assign a value to a the variable NAME llike this:

```bash
NAME='my_name'
AGE=15

# check the variable values using the echo function
echo $NAME
echo $AGE

# what happens if you miss the $ symbol?
echo NAME
echo AGE
```

A variable can also be the output of a command, for example:

```bash
GENUS=$(head -n 10 species.txt | tail -n 1 | sed -r 's/([A-Za-z]+) .+/\1/g' | cut -d' ' -f8)
echo $GENUS
```

A loop is a structure that repeat a set of commands multiple times based on a certain condition. It could be a array of items being exhausted or a variable being equal to a defined value. Two common loops in bioinformatics are `for` and `while`. `for` runs a command in a array of items (variables, files, numbers, etc) until the array is empty.

```bash
for genus in Abacoproeces Agelena Alopecosa Gnaphosa Haplodrassus; do
  echo $genus;
done

# or 

for genus in Abacoproeces Agelena Alopecosa Gnaphosa Haplodrassus; do echo $genus; done
for genus in Abacoproeces Agelena Alopecosa Gnaphosa Haplodrassus; do echo "Genus: $genus"; done
# note above that passing variables and text or passing variables to grep sed, needs double quotes

count=1
while [ $count -le 5 ]; do
    echo "Count is $count";
    ((count++));
done

# above, the -le argument stands for "less than or equal to"
# count++ adds count to count. as count == 1, then it adds 1 at every loop
```

Now, we will use the `genera_filtered.txt` file to search for all [COI - cytochrome c oxidase I](https://en.wikipedia.org/wiki/Cytochrome_c_oxidase_subunit_I) sequences for each genus available on NCBI. We will use a couple of commands from the Blast+ suit, `esearch` and `efetch`. We will learn more about these in more depth in another lecture but for now, `esearch` creates a search query and returns an NCBI URL with the results of the search. `efetch` then takes the URL and retrieves the results in the format chosen by the user, in our case, fasta format.

Note that we are reading `genera_filtered.txt` line by line and asigning the line value to the `$GENUS` variable. Then, we pass that variable to `esearch` and `efetch` to retrieve the sequences in fasta format (we will learn more about sequence formats later).


```bash
while IFS='' read -u 9 -r line; do
    GENUS=$(echo "$line" | cut -f1);
    echo $GENUS;
    grep "$GENUS" spiders.fasta | wc -l
done 9< genera_filtered.txt
```

**Important note:** Keep in mind that different search terms will output different results. For example, searching for "cytochrome" will return sequences with either *cytochrome* or *cytochrome II* or *cytochrome I* in the title and will not return sequences without *cytochrome* but with *COI* in the title.

The code above creates the URL with the results from searching sequences from each genus and with *cytochrome* in the sequence title. Then, one line downloads the results in fasta format (only sequences) and in GeneBank (gb) format.

In fasta files, the line containing the information about the sequence starts with the '>' symbol whereas the lines containing the sequences will not have any symbol at all. Thus, you can count the number of sequences in the fasta file by using `grep` to find the '>' symbol in the `spiders.fasta` file, then pipe the result to `wc` and count the number of lines.

<span style="color: #f08f18;">**Question: **</span>How many sequences are there in the `spiders.fasta` file? Append the answer to your `answersL0_name.txt` using `>>`.

**TO DO:**
Use a `for` loop to grep each genera from the `genera_filtered.txt` file and count how many sequences were downloaded.

<span style="color: #f08f18;">**Question: **</span>Which spider genera do NOT have sequences available? Manually add you answer to your `answersL0_name.txt` using `nano`.

To check the publications associated with the sequence submissions, you can use `grep` and include in the results one extra line for a bit of context. Check for the country and `lat_lon` fields associated with every sequence in the `gb` (or GenBank) file. Where are the vouchers (the source of the sequence) from? Where were they collected? Is any voucher collected in Lithuania?

```bash
grep -A 1 'TITLE' spiders.gb
grep 'country' spiders.gb
grep 'lat_lon' spiders.gb
```

Finally, but not less importantly, we want to understand the quality state of our data. We downloaded COI Sequences from spiders. COI is a universaly used DNA barcode that is sequenced using primers, oligonucleotides that match a conserved region near the sequence of interest that *prime* the taq polymerase to start the synthesis of the complementary strand (after denaturation and annealing). Primers are often sequenced together with the rest of the barcode fragment and it is advisable to remove their sequences from the data before proceeding to do phylogenetic analyses. However, sequences submited to NCBI are not always curated and clean and we must identify which sequences still have the primers. This is easily done with `grep`.

```bash
grep 'AGATATTGG' spiders.fasta # LCO 1490 (Miller et. al., 2013)

# it is possible to search for the complementary and reversed sequence of a primer in the sequences downloaded
# primer Lepidoptera Forward (Hebert et. al., 2004)
# note that the code is not using the pipe but giving three separate instructions
# each instruction is separated by a ;
# this is rarely done because reading the code gets difficult.
# -C provides n lines of context

REVSEQ=$(echo 'TAAAGATAT' | tr ACGTacgt TGCAtgca | rev ); echo $REVSEQ; grep -C 3 "$REVSEQ" spiders.fasta
```

Think about this. What implications exist if the primer sequence is at the begining, at the end, or in the middle of the sequence?

3.5. Compress and dicompress # gzip tar pigz
Compressing and decompressing files is a common task in bioinformatics and it[ consists in reducing the size of a file by reducing the bits needed to encode the same information](https://en.wikipedia.org/wiki/Data_compression). Compressing files is useful because pipelines (a series of interconnected programs/analyses to process data, from raw to final) often generate large files that fill the disk space pretty quickly. It is also useful as smaller files are easier to transfer. Common compression and decompression pairs of programs are (respectively) `gzip` and `gunzip`, and the parallelizable `pigz` and `pigz -dc`. Another popular program is `tar`, which stores multiple files in a single one and has compression options (is not limited to just compress/decompress single files)

Explore the options for each of those programs, particularly how to deal with the output. Which one creates a file automatically? Which one sends the output to *standard output* ([stdout](https://en.wikipedia.org/wiki/Standard_streams)) instead?

3.6. Connecting with other machines # ssh scp rsync
Some times we need to connect to other machines and send files through the command line. Personal computers do not always have enough computational capabilities to handle data but we can connect from our machines to other [High-Performance Computing systems (HPC)](https://en.wikipedia.org/wiki/High-performance_computing) that can handle larger data and more memory-intensive processes. If that is the case, then we will need to transfer large files between local (yours) and remote (the other), something that is not possible through email or simple cloud services.

`ssh` is the most common command for securely login into a remote computer and executing commands on that machine. It generally looks like this:

```bash
ssh username@remote_host

# if we want to specify a specific port, usually port 22
ssh -p 22 username@remote_host
```

Once you are connected, you can run commands just like if you were on your machine, minding the folder structure of the remote machine you are connected to.

`scp` allows you to securely transfer files between your local machine and the remote computer and viceversa. You need to keep in mind where you are (local machine? or already logged into the remote machine?) and the source and destination paths for the files.

```bash
# the command below assumes you are sitting inside the machine from which you are transfering files
# it is usually easier to transfer files from your local machine to remote machines when sitting inside your local machine (password and address)
# copy local file to remote

scp local_file username@remote_host:destination_path/

# to transfer a file from a remote machine to the local machine in which you are sitting
# copy remote file to local
scp username@remote_host:remote_path/file destination_local_folder/
```

`scp` is good enough when files are small to medium files and internet connections are robust. However, `scp` is not enough for transfering larger files over internet connections that might fail and when having to check the integrity of the file after transfer. For that, it is better to use `rsync`. Moreover, if the files exist in both the local and remote machines `rsyn` only transfers the changes between file copies and skips the rest, making the transfer faster.

```bash
# simple transfer from local machine to remote machine
# -a keeps file permissions as they are in the source local machine
# -v verbose, print all output and errors
# -z compress data for transfer (faster)
# after transfer, rsync checks that source and destination files are the same

rsync -avz local_folder/ username@remote_host:destination_path
```

4. AWK
AWK is a programming language design to process text efficiently, particularly tabular text. AWK takes as inputs a pattern, an action, and a file or text from the *standard input* ([stdin](https://en.wikipedia.org/wiki/Standard_streams)) to operate upon. The pattern is a regex expression that indicates a text match or a field in the input file. The action is a command that will be executed on the match.

```bash
awk 'pattern { action }' input_file
```

AWK has pre-defined variables that can be passed as patterns. The most common ones are variables that indicate the entire line of the file `$0` or a particular field (think column) of the file, e.g. `$1` and `$2` for printing the first and second fields/columns of the file. Very similar to the command `cut` would do. As AWK considers fields as variables, it is possible to indicate the delimiter with `-F`.

We will apply AWK commands to `Zelote_coordinates.csv`, a tabular text file that contains geographic coordinates for the spider genera [*Zelotes*](https://en.wikipedia.org/wiki/Zelotes_subterraneus). The data was downloaded on July 26th, 2023 from GBIF and the DOI associated with the search is [here](https://doi.org/10.15468/dl.4ke7tf)

First, use `head`, `cut`, or other commands that you have learnt so far to explore the data within the file.

<span style="color: #f08f18;">**Question: **</span>How many lines are in the file (each line represents a coordinate record)? How many different species are there in the file? Hint: you can use a combination of `head` and `cut` to figure out which file corresponds to the species ephitet. Append your answer to `answersL0_name.txt` using `>>`.

4.1. Basic filtering
Now that you understand what is the structure of the file, the header and name of the columns, and the kind of information that it has, we will use AWK to extract information and clean records with flagged coordinate records. It is possible to carry out the same actions on the file using e.g. Excel, however, there's a limit to the size of files that Excel can process and command-line programs/languages like AWK are faster and more efficient, allowing you to process files that are [gigabytes](https://en.wikipedia.org/wiki/File_size) in size.

First, we will filter out the columns that contain information that is not relevant for us right now. We can use AWK's `print` and field variables to do it. Then, we can add another a filter to only print the lines with non-empty `decimalLatitude` fields, i.e. printing records that are not missing the geolographic coordinates.

```bash
# the new columns are "order family genus species occurrenceStatus individualCount decimalLatitude decimalLongitude issue"
# \t is the regular expression for tab
awk -F '\t' '{print $7, $8, $9, $10, $19, $20, $22, $23, $50}' Zelote_coordinates.csv > Zelote_coordinates_sel.csv

# we are using the \ symbol to introduce a line break without breaking the code (for easy reading)
# OFS sets the output field separator
# in the second part of the pipeline we need to ask awk to BEGIN the program to execute the first action before reading the input file
# the filter is the if conditional and the symbols != mean is different than
# printf prints formated strings
# %s refers to a string match that will be replaced by the variable, in the same order that %s and $var appear
# every %s is separated by a tab symbol and the line ends with the line-end regular expression \n
# we need to do it this way because the column species has a space

awk -F '\t' '{printf ("%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t%s\t\n", $7, $8, $9, $10, $19, $20, $22, $23, $50)}' Zelote_coordinates.csv | \
awk -F '\t' 'BEGIN {OFS = FS} {if ($7 != "") print $0;}' > Zelote_coordinates_sel.csv
```

4.2. Operations
It is possible to caryr out matematichal operations using AWK. The `Zelote_coordinates_sel.csv` column *individualCount* contains the number of spider observations per record. It is common that insects are recorded in counts instead of creating a single record for each insect with the same metadata. Thus, counting lines as records is not enough to count the total number of individuals per species that have been observed. But we can do that with AWK.

AWK has associative arrays, something like a dictionary where the word is a key and the "meaning" is the value. Assuming that we will sum up the individual counts across all the records within a species, then the value of the species column is the key and the individual counts will be the values.

First, familiarise yourself with the *species* and *individualCount* columns, then you can do the sums.

``bash
# we need to skip the header line by asking Number Record higher than one
# && means AND as a conditional such that if species is not empty AND individualCount is not empty
# the information passed to the second pipe only has two columns, species and individualCount
# the last part creates an array called sum from the species column and adds the values in column 2 as a list of values
# AWK ends the first action and starts iterating through the array sum, adding all the individual counts in the list of values

awk '(NR>1)' Zelote_coordinates_sel.csv | awk -F '\t' '{if ($4 != "" && $6 != "") printf "%s\t%s\n", $4, $6;}' | awk -F '\t' '{ sum[$1] += $2 } END { for (group in sum) print group, sum[group] }'
```

The code above then takes a tabular list of geographic records from GBIF, then removes lines with no coordinates or species information, and finally counts how many individual counts in total there are per species.

5. Jupyter notebooks

6. Summary

**Navigating the folders:**
cd: Change directory
ls: List files and directories
pwd: Print working directory

**File and folder manipulation:**
mkdir: Create a new directory
touch: Create a new file
cp: Copy files or directories
mv: Move or rename files or directories
rm: Remove files or directories
grep: Search for a specific pattern in files
find: Search for files and directories

**Permissions:**
chmod: Change file permissions
chown: Change file ownership

**redirecting information and piping:**
>: Redirect output to a file (overwrite)
>>: Redirect output to a file (append)
|: Pipe output of one command as input to another

**Exploring and editing files:**
cat: Concatenate and display file content
less: View file content with pagination
head: Display the beginning of a file
tail: Display the end of a file
nano: Text editor for modifying files

**Data Manipulation:**
awk: Text processing and data extraction
sed: Stream editor for modifying text

**Process management:**
ps: View running processes
kill: Terminate a process

**File compression:**
tar: Archive files and directories
gzip or gunzip: Compress or decompress files

**Remote connections and transfers:**
ssh: Securely connect to a remote server
scp: Securely copy files between local and remote machines
rsync: Securely and efficiently transfer files, more flexible
